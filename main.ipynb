{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset mnist (C:\\Users\\eshaa\\.cache\\huggingface\\datasets\\mnist\\mnist\\1.0.0\\fda16c03c4ecfb13f165ba7e29cf38129ce035011519968cdaf74894ce91c9d4)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.016730546951293945,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e392bb56f60c41d2b1ab77b5ab5b38aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from datasets import load_dataset\n",
    "from random import randint\n",
    "\n",
    "dataset = load_dataset(\"mnist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['image', 'label'],\n",
       "     num_rows: 60000\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['image', 'label'],\n",
       "     num_rows: 10000\n",
       " }))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dataset['train'], dataset['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform MNIST Dataset into variable sized images :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60000/60000 [00:30<00:00, 1942.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting to tensor...Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:05<00:00, 1801.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting to tensor...Done\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def parse_dataset (dataset, desc=\"\"):\n",
    "    oneXImages = []\n",
    "    twoXImages = []\n",
    "    threeXImages = []\n",
    "    labels = []\n",
    "\n",
    "    for value in tqdm(dataset, desc=desc):\n",
    "        oneXImage = value['image']\n",
    "        twoXImage = value['image'].resize((48, 48), resample=Image.BOX)\n",
    "        threeXImage = value['image'].resize((64, 64), resample=Image.BOX)\n",
    "        \n",
    "        oneXImages.append(np.expand_dims(np.uint8(oneXImage),0).tolist())\n",
    "        twoXImages.append(np.expand_dims(np.uint8(twoXImage),0).tolist())\n",
    "        threeXImages.append(np.expand_dims(np.uint8(threeXImage),0).tolist())\n",
    "        labels.append(value['label'])\n",
    "\n",
    "    print(\"Converting to tensor...\",end=\"\") \n",
    "    oneXImages = torch.tensor(oneXImages).to(torch.float)/255\n",
    "    twoXImages = torch.tensor(twoXImages).to(torch.float)/255\n",
    "    threeXImages = torch.tensor(threeXImages).to(torch.float)/255\n",
    "    labels = torch.tensor(labels).to(torch.long)\n",
    "    print(\"Done\")\n",
    "\n",
    "    return oneXImages, twoXImages, threeXImages, labels\n",
    "\n",
    "oneXImagesTrain, twoXImagesTrain, threeXImagesTrain, labelsTrain = parse_dataset(dataset['train'])\n",
    "oneXImagesTest, twoXImagesTest, threeXImagesTest, labelsTest = parse_dataset(dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1x Train Shape:    torch.Size([60000, 1, 28, 28])\n",
      "2x Train Shape:    torch.Size([60000, 1, 48, 48])\n",
      "3x Train Shape:    torch.Size([60000, 1, 64, 64])\n",
      "Train Label Shape: torch.Size([60000])\n",
      "1x Test Shape:     torch.Size([10000, 1, 28, 28])\n",
      "2x Test Shape:     torch.Size([10000, 1, 48, 48])\n",
      "3x Test Shape:     torch.Size([10000, 1, 64, 64])\n",
      "Test Label Shape:  torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "print(\"1x Train Shape:   \", oneXImagesTrain.shape)\n",
    "print(\"2x Train Shape:   \", twoXImagesTrain.shape)\n",
    "print(\"3x Train Shape:   \", threeXImagesTrain.shape)\n",
    "print(\"Train Label Shape:\", labelsTrain.shape)\n",
    "print(\"1x Test Shape:    \", oneXImagesTest.shape)\n",
    "print(\"2x Test Shape:    \", twoXImagesTest.shape)\n",
    "print(\"3x Test Shape:    \", threeXImagesTest.shape)\n",
    "print(\"Test Label Shape: \", labelsTest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Regular NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape: torch.Size([1, 1, 28, 28]) dtype: torch.float32\n",
      "Output shape: torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "# https://medium.com/@nutanbhogendrasharma/pytorch-convolutional-neural-network-with-mnist-dataset-4e8a4265e118\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Sequential(         \n",
    "            nn.Conv2d(\n",
    "                in_channels=1,              \n",
    "                out_channels=16,            \n",
    "                kernel_size=5,              \n",
    "                stride=1,                   \n",
    "                padding=2,                  \n",
    "            ),                              \n",
    "            nn.ReLU(),                      \n",
    "            nn.MaxPool2d(kernel_size=2),    \n",
    "        )\n",
    "        self.conv2 = nn.Sequential(         \n",
    "            nn.Conv2d(\n",
    "                in_channels=16, \n",
    "                out_channels=32, \n",
    "                kernel_size=5, \n",
    "                stride=1,\n",
    "                padding=2),     \n",
    "            nn.ReLU(),                      \n",
    "            nn.MaxPool2d(kernel_size=2),                \n",
    "        )\n",
    "\n",
    "        self.conv3 = nn.Sequential(         \n",
    "            nn.Conv2d(\n",
    "                in_channels=32, \n",
    "                out_channels=32, \n",
    "                kernel_size=5, \n",
    "                stride=1,\n",
    "                padding=2),     \n",
    "            nn.ReLU(),                      \n",
    "            nn.MaxPool2d(kernel_size=2),                \n",
    "        )\n",
    "\n",
    "        # fully connected layer, output 10 classes\n",
    "        self.out = nn.LazyLinear(10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        # flatten the output of conv2 to (batch_size, 32 * 7 * 7)\n",
    "        x = x.view(x.size(0), -1)       \n",
    "        output = self.out(x)\n",
    "        return output\n",
    "\n",
    "model = CNN().to(device)\n",
    "inp = oneXImagesTrain[0].unsqueeze(0).to(device)\n",
    "print(\"Input Shape:\", inp.shape, \"dtype:\", inp.dtype)\n",
    "out = model(inp) # unsqueeze because we only have one sample in the batch\n",
    "print(\"Output shape:\", out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the control model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy on Epoch 1: 11%       \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss: 0.1661: 100%|██████████| 1875/1875 [00:28<00:00, 65.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy on Epoch 2: 95%       \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 2 Loss: 0.0419: 100%|██████████| 1875/1875 [00:31<00:00, 58.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy on Epoch 3: 97%       \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 3 Loss: 0.0013: 100%|██████████| 1875/1875 [00:32<00:00, 57.54it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "from random import randint\n",
    "import numpy as np\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "batch_size = 32\n",
    "epochs = 3\n",
    "\n",
    "losses = []\n",
    "loss_avg = 0\n",
    "for epochs in range(epochs):\n",
    "    # Test Validation\n",
    "    print(\"Running test validation...\", end='\\r')\n",
    "    num_correct = 0\n",
    "    for i in range(0, len(oneXImagesTest), 10):\n",
    "        inp = oneXImagesTest[i:i+10].to(device)\n",
    "        exp_out = labelsTest[i:i+10].to(device)\n",
    "        with torch.no_grad():\n",
    "            out = model(inp)\n",
    "            out = torch.argmax(torch.softmax(out,dim=1),1)\n",
    "        num_correct += torch.sum(exp_out == out).item()\n",
    "    print(f\"Validation Accuracy on Epoch {epochs+1}: {round(num_correct/len(oneXImagesTest)*100)}%       \") \n",
    "\n",
    "    # Actual training\n",
    "    progress_bar = trange(len(oneXImagesTrain)//batch_size)\n",
    "    for i in progress_bar:\n",
    "        index = torch.randperm(len(oneXImagesTrain))[:batch_size]\n",
    "        inp = oneXImagesTrain[index].to(device)\n",
    "        exp_out = labelsTrain[index].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(inp)\n",
    "        loss = loss_func(out, exp_out)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_avg += loss.item()\n",
    "        if i % 100 == 0: \n",
    "            if i == 0:\n",
    "                losses.append(loss.item())\n",
    "            else:\n",
    "                losses.append(loss_avg / 100)\n",
    "                loss_avg = 0\n",
    "        progress_bar.set_description(f\"Epoch: {epochs+1} Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize VNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Input Shape: torch.Size([1, 1, 28, 28]) Output shape: torch.Size([1, 10])\n",
      "Input Shape: torch.Size([1, 1, 48, 48]) Output shape: torch.Size([1, 10])\n",
      "Input Shape: torch.Size([1, 1, 64, 64]) Output shape: torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "from VNN import *\n",
    "\n",
    "class CNNwithVNN (nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Sequential(         \n",
    "            nn.Conv2d(\n",
    "                in_channels=1,              \n",
    "                out_channels=16,            \n",
    "                kernel_size=5,              \n",
    "                stride=1,                   \n",
    "                padding=2,                  \n",
    "            ),                              \n",
    "            nn.ReLU(),                      \n",
    "            nn.MaxPool2d(kernel_size=2),    \n",
    "        )\n",
    "        self.conv2 = nn.Sequential(         \n",
    "            nn.Conv2d(\n",
    "                in_channels=16, \n",
    "                out_channels=32, \n",
    "                kernel_size=5, \n",
    "                stride=1,\n",
    "                padding=2),     \n",
    "            nn.ReLU(),                      \n",
    "            nn.MaxPool2d(kernel_size=2),                \n",
    "        )\n",
    "        self.conv3 = nn.Sequential(         \n",
    "            nn.Conv2d(\n",
    "                in_channels=32, \n",
    "                out_channels=32, \n",
    "                kernel_size=5, \n",
    "                stride=1,\n",
    "                padding=2),     \n",
    "            nn.ReLU(),                      \n",
    "            nn.MaxPool2d(kernel_size=2),                \n",
    "        )\n",
    "\n",
    "        #* VNN Declaration\n",
    "        weight_model = nn.Sequential(\n",
    "            nn.Linear(33, 32),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(32, 1)\n",
    "        ) \n",
    "\n",
    "        bias_model = nn.Sequential(\n",
    "            nn.Linear(17, 10),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(10, 1)\n",
    "        )\n",
    "\n",
    "        dense_model = nn.Sequential(\n",
    "            nn.Linear(128, 10)\n",
    "        )\n",
    "        self.vnn = VNN(dense_model, weight_model, bias_model).to(device)\n",
    "\n",
    "    def forward (self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = x.view(x.size(0), -1)       \n",
    "        x = self.vnn(x)\n",
    "        return x \n",
    "        \n",
    "model = CNNwithVNN().to(device)\n",
    "inp = oneXImagesTrain[0].unsqueeze(0).to(device)\n",
    "out = model(inp)\n",
    "print(f\"Input Shape: {inp.shape} Output shape: {out.shape}\")\n",
    "\n",
    "inp = twoXImagesTrain[0].unsqueeze(0).to(device)\n",
    "out = model(inp)\n",
    "print(f\"Input Shape: {inp.shape} Output shape: {out.shape}\")\n",
    "\n",
    "inp = threeXImagesTrain[0].unsqueeze(0).to(device)\n",
    "out = model(inp)\n",
    "print(f\"Input Shape: {inp.shape} Output shape: {out.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train VNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy on Epoch 1: 10%       \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss: 2.3385:   0%|          | 5/1875 [00:01<10:39,  2.93it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 1024.00 MiB (GPU 0; 6.00 GiB total capacity; 3.37 GiB already allocated; 102.33 MiB free; 3.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\eshaa\\OneDrive\\Desktop\\Coding\\ResearchProject\\main.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/eshaa/OneDrive/Desktop/Coding/ResearchProject/main.ipynb#X21sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m exp_out \u001b[39m=\u001b[39m labelsTrain[index]\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/eshaa/OneDrive/Desktop/Coding/ResearchProject/main.ipynb#X21sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/eshaa/OneDrive/Desktop/Coding/ResearchProject/main.ipynb#X21sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m out \u001b[39m=\u001b[39m model(inp)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/eshaa/OneDrive/Desktop/Coding/ResearchProject/main.ipynb#X21sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_func(out, exp_out)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/eshaa/OneDrive/Desktop/Coding/ResearchProject/main.ipynb#X21sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\eshaa\\OneDrive\\Desktop\\Coding\\ResearchProject\\main.ipynb Cell 14\u001b[0m in \u001b[0;36mCNNwithVNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/eshaa/OneDrive/Desktop/Coding/ResearchProject/main.ipynb#X21sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv3(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/eshaa/OneDrive/Desktop/Coding/ResearchProject/main.ipynb#X21sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mview(x\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)       \n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/eshaa/OneDrive/Desktop/Coding/ResearchProject/main.ipynb#X21sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvnn(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/eshaa/OneDrive/Desktop/Coding/ResearchProject/main.ipynb#X21sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\eshaa\\OneDrive\\Desktop\\Coding\\ResearchProject\\VNN.py:76\u001b[0m, in \u001b[0;36mVNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m (\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> 76\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_weight_vector(x, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfirst_input)\n\u001b[0;32m     77\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdense_nn(x)\n\u001b[0;32m     78\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\eshaa\\OneDrive\\Desktop\\Coding\\ResearchProject\\VNN.py:56\u001b[0m, in \u001b[0;36mVNN.generate_weight_vector\u001b[1;34m(self, x, output_size)\u001b[0m\n\u001b[0;32m     54\u001b[0m argument_two \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_enc(argument_two)\n\u001b[0;32m     55\u001b[0m argument \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mconcat((argument_one, argument_two, x_concat), dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight_nn(argument)\u001b[39m.\u001b[39mview(seq_len, input_size, output_size)\n\u001b[0;32m     57\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mview(seq_len, \u001b[39m1\u001b[39m, input_size)\n\u001b[0;32m     58\u001b[0m out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mbmm(x, weights)\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\activation.py:354\u001b[0m, in \u001b[0;36mTanh.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    353\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 354\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mtanh(\u001b[39minput\u001b[39;49m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1024.00 MiB (GPU 0; 6.00 GiB total capacity; 3.37 GiB already allocated; 102.33 MiB free; 3.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "batch_size = 32\n",
    "epochs = 5\n",
    "\n",
    "losses = []\n",
    "loss_avg = 0\n",
    "for epochs in range(epochs):\n",
    "    # Test Validation\n",
    "    print(\"Running test validation...\", end='\\r')\n",
    "    num_correct = 0\n",
    "    for i in range(0, len(oneXImagesTest), 10):\n",
    "        random_int = randint(0, 2)\n",
    "        if random_int == 0: \n",
    "            inp = oneXImagesTest[i:i+10].to(device)\n",
    "        elif random_int == 1:\n",
    "            inp = twoXImagesTest[i:i+10].to(device)\n",
    "        else:\n",
    "            inp = threeXImagesTest[i:i+10].to(device)\n",
    "\n",
    "        exp_out = labelsTest[i:i+10].to(device)\n",
    "        with torch.no_grad():\n",
    "            out = model(inp)\n",
    "            out = torch.argmax(torch.softmax(out,dim=1),1)\n",
    "        num_correct += torch.sum(exp_out == out).item()\n",
    "    print(f\"Validation Accuracy on Epoch {epochs+1}: {round(num_correct/len(oneXImagesTest)*100)}%       \") \n",
    "\n",
    "    # Actual training\n",
    "    progress_bar = trange(len(oneXImagesTrain)//batch_size)\n",
    "    for i in progress_bar:\n",
    "        index = torch.randperm(len(oneXImagesTrain))[:batch_size]\n",
    "        random_int = randint(0, 2)\n",
    "        if random_int == 0: \n",
    "            inp = oneXImagesTrain[index].to(device)\n",
    "        elif random_int == 1:\n",
    "            inp = twoXImagesTrain[index].to(device)\n",
    "        else:\n",
    "            inp = threeXImagesTrain[index].to(device)\n",
    "            \n",
    "        exp_out = labelsTrain[index].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(inp)\n",
    "        loss = loss_func(out, exp_out)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_avg += loss.item()\n",
    "        if i % 100 == 0: \n",
    "            if i == 0:\n",
    "                losses.append(loss.item())\n",
    "            else:\n",
    "                losses.append(loss_avg / 100)\n",
    "                loss_avg = 0\n",
    "        progress_bar.set_description(f\"Epoch: {epochs+1} Loss: {loss.item():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
