{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eshaa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Reusing dataset mnist (C:\\Users\\eshaa\\.cache\\huggingface\\datasets\\mnist\\mnist\\1.0.0\\fda16c03c4ecfb13f165ba7e29cf38129ce035011519968cdaf74894ce91c9d4)\n",
      "100%|██████████| 2/2 [00:00<00:00, 78.07it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"mnist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['image', 'label'],\n",
       "     num_rows: 60000\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['image', 'label'],\n",
       "     num_rows: 10000\n",
       " }))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'], dataset['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform MNIST Dataset into variable sized images :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resizing x_train dataset:   0%|          | 0/60000 [00:00<?, ?it/s]C:\\Users\\eshaa\\AppData\\Local\\Temp\\ipykernel_9628\\2604634332.py:9: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.\n",
      "  moreUpscaleImage = value['image'].resize((64, 64), resample=Image.BOX)\n",
      "C:\\Users\\eshaa\\AppData\\Local\\Temp\\ipykernel_9628\\2604634332.py:10: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.\n",
      "  upscaleImage = value['image'].resize((48, 48), resample=Image.BOX)\n",
      "Resizing x_train dataset: 100%|██████████| 60000/60000 [00:11<00:00, 5007.67it/s]\n",
      "Resizing x_test dataset:   0%|          | 0/10000 [00:00<?, ?it/s]C:\\Users\\eshaa\\AppData\\Local\\Temp\\ipykernel_9628\\2604634332.py:24: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.\n",
      "  moreUpscaleImage = value['image'].resize((64, 64), resample=Image.BOX)\n",
      "C:\\Users\\eshaa\\AppData\\Local\\Temp\\ipykernel_9628\\2604634332.py:25: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.\n",
      "  upscaleImage = value['image'].resize((48, 48), resample=Image.BOX)\n",
      "Resizing x_test dataset: 100%|██████████| 10000/10000 [00:01<00:00, 5223.22it/s]\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "for value in tqdm(dataset['train'], desc=\"Resizing x_train dataset\"):\n",
    "    moreUpscaleImage = value['image'].resize((64, 64), resample=Image.BOX)\n",
    "    upscaleImage = value['image'].resize((48, 48), resample=Image.BOX)\n",
    "    regularImage = value['image']\n",
    "\n",
    "    moreUpscaleImage = torch.from_numpy(np.uint8(moreUpscaleImage)).flatten()\n",
    "    upscaleImage = torch.from_numpy(np.uint8(upscaleImage)).flatten()\n",
    "    regularImage = torch.from_numpy(np.uint8(regularImage)).flatten()\n",
    "\n",
    "    x_train.extend([regularImage, upscaleImage, moreUpscaleImage]) \n",
    "    y_train.extend([value['label'], value['label'], value['label']])\n",
    "\n",
    "x_test = []\n",
    "y_test = []\n",
    "\n",
    "for value in tqdm(dataset['test'], desc=\"Resizing x_test dataset\"):\n",
    "    moreUpscaleImage = value['image'].resize((64, 64), resample=Image.BOX)\n",
    "    upscaleImage = value['image'].resize((48, 48), resample=Image.BOX)\n",
    "    regularImage = value['image']\n",
    "\n",
    "    moreUpscaleImage = torch.from_numpy(np.uint8(moreUpscaleImage)).flatten()\n",
    "    upscaleImage = torch.from_numpy(np.uint8(upscaleImage)).flatten()\n",
    "    regularImage = torch.from_numpy(np.uint8(regularImage)).flatten()\n",
    "\n",
    "    x_test.extend([regularImage, upscaleImage, moreUpscaleImage]) \n",
    "    y_test.extend([value['label'], value['label'], value['label']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audioop import bias\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        return self.pe[x].squeeze(1)\n",
    "\n",
    "class VNN (nn.Module):\n",
    "    def __init__(self, dense_nn, weight_nn, bias_nn) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = (weight_nn[0].in_features-1)//2\n",
    "        self.first_input = dense_nn[0].in_features\n",
    "\n",
    "        self.dense_nn = dense_nn\n",
    "        self.weight_nn = weight_nn \n",
    "        self.bias_nn = bias_nn \n",
    "        self.pos_enc = PositionalEncoding(self.d_model, dropout=0.1, max_len=5000)\n",
    "\n",
    "    def generate_weight_vector (self, x, output_size):\n",
    "        input_size = x.size(0)\n",
    "\n",
    "        #* Weight Generation\n",
    "\n",
    "        # Generate the weight vector\n",
    "        argument_one = torch.arange(input_size).unsqueeze(1)\n",
    "        \n",
    "        argument_two = torch.arange(output_size)\n",
    "        bias_argument = deepcopy(argument_two) \n",
    "        # Generate the repeat\n",
    "        argument_one = argument_one.repeat(1, output_size).flatten()\n",
    "        argument_two = argument_two.repeat(input_size)\n",
    "\n",
    "        x_concat = x[argument_one].unsqueeze(1)\n",
    "    \n",
    "        # Positional Encoding + Concat\n",
    "        argument_one = self.pos_enc(argument_one)\n",
    "        argument_two = self.pos_enc(argument_two)\n",
    "        bias_argument = self.pos_enc(bias_argument) \n",
    "        argument = torch.concat((argument_one, argument_two, x_concat), dim=1)\n",
    "\n",
    "        # Send through the weight neural network\n",
    "        weights = self.weight_nn(argument).view(input_size, output_size)\n",
    "\n",
    "        out = torch.matmul(x, weights).unsqueeze(1)\n",
    "\n",
    "        #* Bias Generation \n",
    "        argument = torch.concat((bias_argument, out), dim=1)\n",
    "        bias = self.bias_nn(argument)\n",
    "        out += bias\n",
    "\n",
    "        return out.squeeze(1)\n",
    "\n",
    "    def forward (self, x):\n",
    "        x = self.generate_weight_vector(x, self.first_input)\n",
    "        x = self.dense_nn(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eshaa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\container.py:139: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.0923, 0.0600, 0.1448, 0.1391, 0.1057, 0.0664, 0.1063, 0.1071, 0.0663,\n",
       "        0.1120], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_model = 32\n",
    "weight_model = nn.Sequential(\n",
    "    nn.Linear(65, 32),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(32, 1)\n",
    ") \n",
    "\n",
    "bias_model = nn.Sequential(\n",
    "    nn.Linear(33, 10),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(10, 1)\n",
    ")\n",
    "\n",
    "dense_model = nn.Sequential(\n",
    "    nn.Linear(128, 64),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(64,  10),\n",
    "    nn.Softmax()\n",
    ")\n",
    "\n",
    "policy = VNN(dense_model, weight_model, bias_model)\n",
    "policy(torch.randn(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/180000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], dtype=torch.uint8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "from random import randint\n",
    "import numpy as np\n",
    "\n",
    "for _ in trange(len(x_train)):\n",
    "    index = randint(0, len(x_train)-1)\n",
    "    print(x_train[index])\n",
    "    break\n",
    "    \n",
    "    pass\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "639d95488edfdc55ddccb2f409d15bacc5e558fc037d9d032accd1031f2e88a9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
